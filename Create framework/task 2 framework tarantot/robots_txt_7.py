"""
Robots.txt — это текстовый файл, который содержит параметры индексирования
сайта для роботов поисковых систем. Зачастую в этом файле можно обнаружить
директории и файлы запрещённые к индексированию, и среди них скрытые, что
может потенциально упростить взлом.
"""
import requests
import re


def get_page_data_robots():
    url = input('Enter host [https://site.com]: ')
    head = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/65.0.3325.181 Safari/537.36"}

    while not re.fullmatch(r'https?://[\d\-_.~a-z]+\.[a-z]{2,}', url):
        print('\nPlease enter a host by the pattern [https://site.com] !!!\n')
        url = input('Enter host [https://site.com]: ')
    else:
        try:
            if url[-1] == '/':
                page = requests.get(f'{url}robots.txt', headers=head)
            else:
                page = requests.get(f'{url}/robots.txt', headers=head)
        except requests.exceptions.ConnectionError:
            return f'\nThe website {url} not found.'
        except Exception as e:
            return f'\n{url} ERROR --- {e}'
        else:
            if page.status_code != 404:
                return page.text
            else:
                return f'File "robots.txt" not found for {url}!'